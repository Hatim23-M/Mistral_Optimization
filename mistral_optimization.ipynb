{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom huggingface_hub import login\nimport time\nimport threading\nimport torch.nn.utils.prune as prune\nimport onnx\nfrom torch.nn.utils import prune\nimport torch.nn as nn\nimport os","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logging in to the hugging face network usign my authorised token to access the model. Enter your access key before moving onto the next cells\nlogin('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')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Creating a function to load the pretrained mistral model from huggingface using the AutoTokenizer and AutoModelforCausalLM methods provided by the transformers library.","metadata":{}},{"cell_type":"code","source":"def load_model(model_path):\n    \n    # Loading the model\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForCausalLM.from_pretrained(model_path)\n    \n    return model, tokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Creating a function to optimize the model to better fit it within the limitations of this notebook","metadata":{}},{"cell_type":"code","source":"def optimize_model(model, dtype=torch.float16):\n    # Setting the capacity of the model to half its strength\n    model = model.half() if dtype == torch.float16 else model\n    # Moving the model to the GPU\n    model = model.cuda()\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prune_model_stepwise(model, prune_ratio=0.2, step_size=0.0001):\n    \"\"\"\n    Prune the model in steps to manage GPU memory usage.\n    Args:\n        model: The model to be pruned.\n        prune_ratio: Total amount of pruning to be applied.\n        step_size: Fraction of pruning to be applied in each step.\n    Returns:\n        The pruned model.\n    \"\"\"\n    # Get a list of all prunable layers\n    prunable_layers = [(name, module) for name, module in model.named_modules() if isinstance(module, nn.Linear)]\n    \n    # Calculate the number of steps\n    total_layers = len(prunable_layers)\n    steps = int(prune_ratio / step_size)\n    \n    for step in range(steps):\n        print(f\"Step {step + 1}/{steps}\")\n        # Calculate the amount to prune in this step\n        prune_amount = step_size\n        \n        for name, module in prunable_layers:\n            # Apply pruning to the layer\n            prune.l1_unstructured(module, 'weight', amount=prune_amount)\n            prune.remove(module, 'weight')\n            torch.cuda.empty_cache()  # Clear GPU cache after each layer\n        \n        # Optional: Validate the model after each step\n        # validate_model(model)\n        \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prune_layer(module, amount):\n    try:\n        prune.l1_unstructured(module, 'weight', amount=amount)\n        prune.remove(module, 'weight')\n        torch.cuda.empty_cache()  # Clear GPU cache\n    except RuntimeError as e:\n        print(f\"Error during pruning: {e}\")\n        torch.cuda.empty_cache()  # Clear GPU cache and try again\n\ndef prune_model_stepwise_with_offloading(model, prune_ratio=0.2, step_size=0.1):\n    \"\"\"\n    Prune the model in steps to manage GPU memory usage.\n    Args:\n        model: The model to be pruned.\n        prune_ratio: Total amount of pruning to be applied.\n        step_size: Fraction of pruning to be applied in each step.\n    Returns:\n        The pruned model.\n    \"\"\"\n    # Set PyTorch memory management configuration\n    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:32'\n\n    # Get a list of all prunable layers\n    prunable_layers = [(name, module) for name, module in model.named_modules() if isinstance(module, nn.Linear)]\n    \n    # Calculate the number of steps\n    steps = int(prune_ratio / step_size)\n    \n    for step in range(steps):\n        print(f\"Step {step + 1}/{steps}\")\n        # Calculate the amount to prune in this step\n        prune_amount = step_size\n        \n        for name, module in prunable_layers:\n            # Skip if the layer is already pruned\n            if not hasattr(module, 'weight_orig'):\n                # Move the module to GPU\n                module = module.to(torch.float16).cuda()\n                # Offload the rest of the model to CPU\n                for other_name, other_module in model.named_modules():\n                    if other_name != name:\n                        other_module.cpu()\n                torch.cuda.empty_cache()  # Clear GPU cache\n\n                # Prune the current module\n                prune_layer(module, prune_amount)\n\n                # Convert back to half precision if needed\n                module = module.to(torch.float16).cpu()\n                torch.cuda.empty_cache()  # Clear GPU cache\n\n    # Move the entire model back to GPU\n    model.cuda()\n    torch.cuda.empty_cache()  # Clear GPU cache\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Creating the function 'infer' that takes in the user input and generates the output by the model","metadata":{}},{"cell_type":"code","source":"# 'infer' function generates responses to a given prompt\ndef infer(model, tokenizer, prompt, max_length=128):\n    \n    # Tokenizing the input prompt\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    \n    # Generating a response from the model\n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_length=max_length)\n        \n    # Decoding the generated tokens back to the readable text\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    return response","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Creating a function to evaluate the performance metrics of the model.","metadata":{}},{"cell_type":"code","source":"# 'infer' function generates responses to a given prompt\ndef infer2(model, tokenizer, prompt, max_length=128):\n    \n    # Tokenizing the input prompt\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    \n    # Generating a response from the model\n    with torch.no_grad():\n        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n            outputs = model.generate(**inputs, max_length=max_length)\n        \n    # Decoding the generated tokens back to the readable text\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    return response","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def async_infer(model, tokenizer, prompt, max_length=128):\n    stream = torch.cuda.Stream()\n    with torch.cuda.stream(stream):\n        response = infer2(model, tokenizer, prompt, max_length)\n        \n    return response","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def benchmark(model, tokenizer, prompt, num_runs=1, concurrency=1):\n    times = []\n    \n    for i in range(num_runs):\n        start_time = time.time()\n        batch_prompts = [prompt for _ in range(concurrency)]\n        responses = concurrent_infer(model, tokenizer, batch_prompts)\n        end_time = time.time()\n        \n        times.append(end_time-start_time)\n    \n    avg_time = sum(times)/num_runs\n    throughput = (256*concurrency)/avg_time\n    \n    return avg_time, throughput","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def concurrent_infer(model, tokenizer, prompts, max_length=128):\n    def worker(prompt, results, idx):\n        results[idx] = infer2(model, tokenizer, prompt, max_length)\n    \n    threads = []\n    results = [None]*len(prompts)\n    for i, prompt in enumerate(prompts):\n        print(worker)\n        thread = threading.Thread(target=worker, args=(prompt, results, i))\n        threads.append(thread)\n        thread.start()\n        \n    for thread in threads:\n        thread.join()\n    \n    return results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining the model path\nmodel_path = \"mistralai/Mistral-7B-Instruct-v0.3\"\n\n# Loading the model\nmodel, tokenizer = load_model(model_path)\n\n#Optimizing the model\nmodel_2 = optimize_model(model, dtype=torch.float16)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# User input\nprompt = input(\"Enter your prompt: \")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting the response\nresponse = infer2(model_2, tokenizer, prompt)\n\nprint(\"Model response: \", response)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_3 = prune_model_stepwise_with_offloading(model_2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Benchmark the model performance\navg_time, throughput = benchmark(model_3, tokenizer, prompt)\n\nprint(\"Average inference time: \", avg_time)\nprint(f'Throughput: {throughput} tokens/sec')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}