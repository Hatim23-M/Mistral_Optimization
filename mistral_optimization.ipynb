{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from huggingface_hub import login\n","import time\n","import threading\n","import torch.nn.utils.prune as prune\n","import onnx\n","from torch.nn.utils import prune\n","import torch.nn as nn\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Logging in to the hugging face network usign my authorised token to access the model. Enter your access key before moving onto the next cells\n","login(\"!!!! ENTER YOUR LOGIN KEY HERE !!!!\")"]},{"cell_type":"markdown","metadata":{},"source":["#### Creating a function to load the pretrained mistral model from huggingface using the AutoTokenizer and AutoModelforCausalLM methods provided by the transformers library."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def load_model(model_path):\n","    \n","    # Loading the model\n","    tokenizer = AutoTokenizer.from_pretrained(model_path)\n","    model = AutoModelForCausalLM.from_pretrained(model_path)\n","    \n","    return model, tokenizer"]},{"cell_type":"markdown","metadata":{},"source":["#### Creating a function to optimize the model to better fit it within the limitations of this notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def optimize_model(model, dtype=torch.float16):\n","    # Setting the capacity of the model to half its strength\n","    model = model.half() if dtype == torch.float16 else model\n","    # Moving the model to the GPU\n","    model = model.cuda()\n","    \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def prune_model_stepwise(model, prune_ratio=0.2, step_size=0.0001):\n","    \"\"\"\n","    Prune the model in steps to manage GPU memory usage.\n","    Args:\n","        model: The model to be pruned.\n","        prune_ratio: Total amount of pruning to be applied.\n","        step_size: Fraction of pruning to be applied in each step.\n","    Returns:\n","        The pruned model.\n","    \"\"\"\n","    # Get a list of all prunable layers\n","    prunable_layers = [(name, module) for name, module in model.named_modules() if isinstance(module, nn.Linear)]\n","    \n","    # Calculate the number of steps\n","    total_layers = len(prunable_layers)\n","    steps = int(prune_ratio / step_size)\n","    \n","    for step in range(steps):\n","        print(f\"Step {step + 1}/{steps}\")\n","        # Calculate the amount to prune in this step\n","        prune_amount = step_size\n","        \n","        for name, module in prunable_layers:\n","            # Apply pruning to the layer\n","            prune.l1_unstructured(module, 'weight', amount=prune_amount)\n","            prune.remove(module, 'weight')\n","            torch.cuda.empty_cache()  # Clear GPU cache after each layer\n","        \n","        # Optional: Validate the model after each step\n","        # validate_model(model)\n","        \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def prune_layer(module, amount):\n","    try:\n","        prune.l1_unstructured(module, 'weight', amount=amount)\n","        prune.remove(module, 'weight')\n","        torch.cuda.empty_cache()  # Clear GPU cache\n","    except RuntimeError as e:\n","        print(f\"Error during pruning: {e}\")\n","        torch.cuda.empty_cache()  # Clear GPU cache and try again\n","\n","def prune_model_stepwise_with_offloading(model, prune_ratio=0.2, step_size=0.1):\n","    \"\"\"\n","    Prune the model in steps to manage GPU memory usage.\n","    Args:\n","        model: The model to be pruned.\n","        prune_ratio: Total amount of pruning to be applied.\n","        step_size: Fraction of pruning to be applied in each step.\n","    Returns:\n","        The pruned model.\n","    \"\"\"\n","    # Set PyTorch memory management configuration\n","    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:32'\n","\n","    # Get a list of all prunable layers\n","    prunable_layers = [(name, module) for name, module in model.named_modules() if isinstance(module, nn.Linear)]\n","    \n","    # Calculate the number of steps\n","    steps = int(prune_ratio / step_size)\n","    \n","    for step in range(steps):\n","        print(f\"Step {step + 1}/{steps}\")\n","        # Calculate the amount to prune in this step\n","        prune_amount = step_size\n","        \n","        for name, module in prunable_layers:\n","            # Skip if the layer is already pruned\n","            if not hasattr(module, 'weight_orig'):\n","                # Move the module to GPU\n","                module = module.to(torch.float16).cuda()\n","                # Offload the rest of the model to CPU\n","                for other_name, other_module in model.named_modules():\n","                    if other_name != name:\n","                        other_module.cpu()\n","                torch.cuda.empty_cache()  # Clear GPU cache\n","\n","                # Prune the current module\n","                prune_layer(module, prune_amount)\n","\n","                # Convert back to half precision if needed\n","                module = module.to(torch.float16).cpu()\n","                torch.cuda.empty_cache()  # Clear GPU cache\n","\n","    # Move the entire model back to GPU\n","    model.cuda()\n","    torch.cuda.empty_cache()  # Clear GPU cache\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["#### Creating the function 'infer' that takes in the user input and generates the output by the model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# 'infer' function generates responses to a given prompt\n","def infer(model, tokenizer, prompt, max_length=128):\n","    \n","    # Tokenizing the input prompt\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n","    \n","    # Generating a response from the model\n","    with torch.no_grad():\n","        outputs = model.generate(**inputs, max_length=max_length)\n","        \n","    # Decoding the generated tokens back to the readable text\n","    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    \n","    return response"]},{"cell_type":"markdown","metadata":{},"source":["#### Creating a function to evaluate the performance metrics of the model."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# 'infer' function generates responses to a given prompt\n","def infer2(model, tokenizer, prompt, max_length=128):\n","    \n","    # Tokenizing the input prompt\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n","    \n","    # Generating a response from the model\n","    with torch.no_grad():\n","        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n","            outputs = model.generate(**inputs, max_length=max_length)\n","        \n","    # Decoding the generated tokens back to the readable text\n","    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    \n","    return response"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def async_infer(model, tokenizer, prompt, max_length=128):\n","    stream = torch.cuda.Stream()\n","    with torch.cuda.stream(stream):\n","        response = infer2(model, tokenizer, prompt, max_length)\n","        \n","    return response"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def benchmark(model, tokenizer, prompt, num_runs=1, concurrency=1):\n","    times = []\n","    \n","    for i in range(num_runs):\n","        start_time = time.time()\n","        batch_prompts = [prompt for _ in range(concurrency)]\n","        responses = concurrent_infer(model, tokenizer, batch_prompts)\n","        end_time = time.time()\n","        \n","        times.append(end_time-start_time)\n","    \n","    avg_time = sum(times)/num_runs\n","    throughput = (256*concurrency)/avg_time\n","    \n","    return avg_time, throughput"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def concurrent_infer(model, tokenizer, prompts, max_length=128):\n","    def worker(prompt, results, idx):\n","        results[idx] = infer2(model, tokenizer, prompt, max_length)\n","    \n","    threads = []\n","    results = [None]*len(prompts)\n","    for i, prompt in enumerate(prompts):\n","        print(worker)\n","        thread = threading.Thread(target=worker, args=(prompt, results, i))\n","        threads.append(thread)\n","        thread.start()\n","        \n","    for thread in threads:\n","        thread.join()\n","    \n","    return results"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Defining the model path\n","model_path = \"mistralai/Mistral-7B-Instruct-v0.3\"\n","\n","# Loading the model\n","model, tokenizer = load_model(model_path)\n","\n","#Optimizing the model\n","model_2 = optimize_model(model, dtype=torch.float16)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# User input\n","prompt = input(\"Enter your prompt: \")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Getting the response\n","response = infer2(model_2, tokenizer, prompt)\n","\n","print(\"Model response: \", response)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_3 = prune_model_stepwise_with_offloading(model_2)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Benchmark the model performance\n","avg_time, throughput = benchmark(model_3, tokenizer, prompt)\n","\n","print(\"Average inference time: \", avg_time)\n","print(f'Throughput: {throughput} tokens/sec')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!nvidia-smi\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
